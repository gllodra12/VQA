{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a variational quantum circuit with Pennylane\n",
    "\n",
    "In this notebook, we will learn how to build and train a variational quantum circuit (VQA) to generate a predefined 4 qubit target state from a 4 qubit random state of our choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Given the following predefined 4 qubit states: $\\lvert 0011 \\rangle$, $\\lvert 0101 \\rangle$, $\\lvert 1010 \\rangle$, $\\lvert 1100 \\rangle$. Our goal is to create a quantum circuit that can do the following mapping:\n",
    "\n",
    "- Initial random state 1, returns $\\lvert 0011 \\rangle$.\n",
    "- Initial random state 2, returns $\\lvert 0101 \\rangle$.\n",
    "- Initial random state 3, returns $\\lvert 1010 \\rangle$.\n",
    "- Initial random state 4, returns $\\lvert 1100 \\rangle$.\n",
    "\n",
    "However, for the sake of simplicity in the first section, I am going to also predefine the initial random state. In the second section, we will see how the code in the first section can be extended to solve the initial statement.\n",
    "\n",
    "$$\\lvert 0001 \\rangle \\rightarrow  \\lvert 0011 \\rangle $$\n",
    "$$\\lvert 0010 \\rangle \\rightarrow  \\lvert 0101 \\rangle $$\n",
    "$$\\lvert 0100 \\rangle \\rightarrow  \\lvert 1010 \\rangle $$\n",
    "$$\\lvert 1000 \\rangle \\rightarrow  \\lvert 1100 \\rangle $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = [\n",
    "    np.array([0, 0, 0, 1]),\n",
    "    np.array([0, 0, 1, 0]),\n",
    "    np.array([0, 1, 0, 0]),\n",
    "    np.array([1, 0, 0, 0])\n",
    "]\n",
    "\n",
    "\n",
    "target_states = [\n",
    "    np.array([0, 0, 1, 1]),\n",
    "    np.array([0, 1, 0, 1]),\n",
    "    np.array([1, 0, 1, 0]),\n",
    "    np.array([1, 1, 0, 0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a circuit\n",
    "\n",
    "The most difficult part of defining a circuit is to choose a good ansatz. Since I don't have a clue of what will work best I will implement a general circuit parametrized. Using 2 rotational qubit gates (Rx and Ry) I can generate all possible states in the Bloch Sphere. Then, I will implement several CNOT gates to entangle all the qubits. \n",
    "\n",
    "I will repeat this layer (rotation gates + CNOT gates) 2 times. The number of layers is a hyperparameter that we may need to tune later.\n",
    "\n",
    "**Important:**  We need to set the parameters for the rotational gates, since I want to generate all possible angles, I will generate random numbers from $0$ to $\\pi$.\n",
    "\n",
    "<font color=red>'NOT COMPLETLY SURE'</font>  \n",
    "In the [tutorial](https://pennylane.ai/qml/demos/tutorial_state_preparation.html) generates random numbers from 0 to pi. This is enough to generat all possible states in the bloch sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "n_rotations = 2\n",
    "\n",
    "# randomly initialize parameters from a normal distribution\n",
    "params = np.random.normal(0, np.pi, (n_qubits, n_layers, n_rotations))\n",
    "params = Variable(torch.tensor(params), requires_grad=True)\n",
    "\n",
    "def circuit_layer(n_qubits: int , params: np.array, layer_lvl: int):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(params[i, layer_lvl, 0], wires=i)\n",
    "        qml.RY(params[i, layer_lvl, 1], wires=i)\n",
    "\n",
    "    # Entangling all qubits\n",
    "    qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our circuit in 3 steps: \n",
    "\n",
    "\n",
    "**1. Initialize in the right state.**  \n",
    "All circuits start with all qubits in the zero state, $\\lvert 0\\rangle$. Using ```qml.BasisState``` our circuit will start with the state of our choice: $\\lvert 0001 \\rangle$ or $\\lvert 0010 \\rangle$, etc.\n",
    "\n",
    "**2. Implement the layer.**  \n",
    "Using a simple for loop we can add as many layers as we want.\n",
    "\n",
    "**3. Measurement.**    \n",
    "In this last step, we will measure the **fidelity** between the **output state** and **target state**. The fidelity is a value between 0 (orthogonal) or 1 (equal) that measure the closeness between two states, in other words, it measures if both states are similar or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def circuit(params, initial_state, fidelity_op, n_layers):\n",
    "    n_qubits = len(initial_state)\n",
    "\n",
    "    qml.BasisState(initial_state, wires=range(n_qubits))\n",
    "    \n",
    "    for layer_lvl in range(n_layers):\n",
    "        circuit_layer(n_qubits, params, layer_lvl)\n",
    "\n",
    "    return qml.expval(qml.Hermitian(fidelity_op, wires=range(n_qubits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a cost function\n",
    "\n",
    "The cost function helps the circuit to find the target state. In this case, our cost function will be the fidelity, so we can simply define our cost function as:\n",
    "\n",
    "$$ C(\\theta) = \\sum \\big( 1-\\langle \\psi_o(\\theta) \\lvert \\psi_t \\rangle \\big) ^2 $$\n",
    "\n",
    "where $\\theta$ are the variational parameters that the circuit needs to tune, $\\psi_o$ is the output state, $\\psi_t$ is one of the target states that we have define in the begining and $\\langle \\psi_o(\\theta) \\lvert \\psi_t \\rangle$ is how the fidelity is computed.\n",
    "\n",
    "\n",
    "### Explaining fidelity_op (Extra)\n",
    "\n",
    "_I have written this subsection in case someone needs to understand how to implement the fidelity operator._\n",
    "\n",
    "Suppose I have the inital state $\\lvert 00 \\rangle$ and our target is $\\lvert 01 \\rangle$. If our quantum circuit doesn't works properly it can outputs states like:\n",
    "- $\\lvert 10 \\rangle$\n",
    "- $\\lvert 11 \\rangle$.  \n",
    "\n",
    "But which state is closer to the target? In order to answer this question we can compute the fidelity operator of our target state ($\\lvert 01 \\rangle \\langle 01 \\rvert$).\n",
    "$$ \\lvert 01 \\rangle \\langle 01 \\rvert = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "And measure how close each state is:  \n",
    "**State  $\\lvert 10 \\rangle$**:\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "0  \\\\\n",
    "\\end{pmatrix}\n",
    "= 0$$ \n",
    "\n",
    "**State  $\\lvert 11 \\rangle$**:\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{pmatrix}\n",
    "= 0$$ \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Even though state $\\lvert 11 \\rangle$ is closer to $\\lvert 01 \\rangle$ (there is only 1 qubit wrong), the fidelity interprets that both $\\lvert 11 \\rangle$ and $\\lvert 10 \\rangle$ are far away from $\\lvert 01 \\rangle$.\n",
    "\n",
    "<font color=red>'ASK'</font>  \n",
    "_Perhaps the fidelity is not a good cost function for this case. <font color=red>Future myself answer this question</font>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_product(matrices: list):\n",
    "    if len(matrices)==2:\n",
    "        return np.kron(matrices[0], matrices[1], requires_grad=False)\n",
    "    return np.kron(matrices[0], tensor_product(matrices[1:]), requires_grad=False)\n",
    "\n",
    "\n",
    "def density_matrix(state: np.array):\n",
    "    # Define |0> and |1>\n",
    "    zero_state = np.array([1, 0], requires_grad=False)\n",
    "    one_state = np.array([0, 1], requires_grad=False)\n",
    "\n",
    "    # Computes density matrix\n",
    "    matrices = [zero_state if qubit_state == 0 else one_state for qubit_state in state]\n",
    "    vector = tensor_product(matrices)\n",
    "    return np.outer(vector, np.conj(vector), requires_grad=False)\n",
    "\n",
    "\n",
    "def cost(params, initial_states, target_states, n_layers):\n",
    "    loss = 0\n",
    "    fidelity_operators = [density_matrix(i) for i in target_states]\n",
    "    for i in range(len(target_states)):\n",
    "        fidelity = circuit(params, initial_states[i], fidelity_operators[i], n_layers)\n",
    "        loss += (1 - fidelity) ** 2\n",
    "\n",
    "    return loss / len(target_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best parameters I use the code that you can find in this [tutorial](https://pennylane.ai/qml/demos/tutorial_state_preparation.html). Using a classical optimizer we can find the best parameters that minimize the previous cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 steps is 0.8170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/gllodra12/extended_linux/Projectes/Quantum_projects/qosf/qosf_2021_09/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:147: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:240.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 10 steps is 0.6030\n",
      "Cost after 20 steps is 0.3905\n",
      "Cost after 30 steps is 0.3942\n",
      "Cost after 40 steps is 0.3764\n",
      "Cost after 50 steps is 0.3772\n",
      "Cost after 60 steps is 0.3754\n",
      "Cost after 70 steps is 0.3752\n",
      "Cost after 80 steps is 0.3751\n",
      "Cost after 90 steps is 0.3750\n",
      "Cost after 100 steps is 0.3750\n",
      "Cost after 110 steps is 0.3750\n",
      "Cost after 120 steps is 0.3750\n",
      "Cost after 130 steps is 0.3750\n",
      "Cost after 140 steps is 0.3750\n",
      "Cost after 150 steps is 0.3750\n",
      "Cost after 160 steps is 0.3750\n",
      "Cost after 170 steps is 0.3750\n",
      "Cost after 180 steps is 0.3750\n",
      "Cost after 190 steps is 0.3750\n",
      "Cost after 200 steps is 0.3750\n"
     ]
    }
   ],
   "source": [
    "# set up the optimizer\n",
    "opt = torch.optim.Adam([params], lr=0.1)\n",
    "\n",
    "# number of steps in the optimization routine\n",
    "steps = 200\n",
    "\n",
    "# the final stage of optimization isn't always the best, so we keep track of\n",
    "# the best parameters along the way\n",
    "best_cost = cost(params, initial_states, target_states, n_layers)\n",
    "best_params = np.zeros((n_qubits, n_layers, n_rotations))\n",
    "\n",
    "print(\"Cost after 0 steps is {:.4f}\".format(best_cost))\n",
    "\n",
    "# optimization begins\n",
    "for n in range(steps):\n",
    "    opt.zero_grad()\n",
    "    loss = cost(params, initial_states, target_states, n_layers)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # keeps track of best parameters\n",
    "    if loss < best_cost:\n",
    "        best_cost = loss\n",
    "        best_params = params\n",
    "\n",
    "    # Keep track of progress every 10 steps\n",
    "    if n % 10 == 9 or n == steps - 1:\n",
    "        print(\"Cost after {} steps is {:.4f}\".format(n + 1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results don't look really promising, perhaps we will need to increase the number of layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the initial state [0 0 0 1] we get an output with fidelity 0.0000\n",
      "From the initial state [0 0 1 0] we get an output with fidelity 0.5000\n",
      "From the initial state [0 1 0 0] we get an output with fidelity 0.5000\n",
      "From the initial state [1 0 0 0] we get an output with fidelity 0.9993\n"
     ]
    }
   ],
   "source": [
    "def check_results(best_params, initial_states, target_states, n_layers):\n",
    "    fidelity_operators = [density_matrix(i) for i in target_states]\n",
    "    \n",
    "    for initial_state, fidelity_op in zip(initial_states, fidelity_operators):\n",
    "        fidelity = circuit(best_params, initial_state, fidelity_op, n_layers)\n",
    "        print(\"From the initial state {} we get an output with fidelity {:.4f}\".format(initial_state, fidelity))\n",
    "\n",
    "check_results(best_params, initial_states, target_states, n_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion 1st circuit\n",
    "\n",
    "Our circuit is not able to generate the target states. We only get a good result with the input $\\lvert 1000 \\rangle$ because its fidelity is close to 1, which means that the output state is really close to the target state, $\\lvert 1100 \\rangle$.\n",
    "\n",
    "Let's see if increasing the number of layers our results will improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(initial_states, target_states, n_layers, verbose=True):\n",
    "    n_qubits = initial_states[0].size\n",
    "\n",
    "    # Total of rotations gates (Rx and Ry)\n",
    "    n_rotations = 2\n",
    "    \n",
    "    # Setting intial parameters\n",
    "    params = np.random.normal(0, np.pi, (n_qubits, n_layers, n_rotations))\n",
    "    params = Variable(torch.tensor(params), requires_grad=True)\n",
    "    \n",
    "    # Computing the best parameters\n",
    "    opt = torch.optim.Adam([params], lr=0.1)\n",
    "\n",
    "    # number of steps in the optimization routine\n",
    "    steps = 200\n",
    "\n",
    "    # the final stage of optimization isn't always the best, so we keep track of the best parameters along the way\n",
    "    best_cost = cost(params, initial_states, target_states, n_layers)\n",
    "    best_params = np.zeros((n_qubits, n_layers, n_rotations))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Cost after 0 steps is {:.4f}\".format(best_cost))\n",
    "\n",
    "    # optimization begins\n",
    "    for n in range(steps):\n",
    "        opt.zero_grad()\n",
    "        loss = cost(params, initial_states, target_states, n_layers)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keeps track of best parameters\n",
    "        if loss < best_cost:\n",
    "            best_cost = loss\n",
    "            best_params = params\n",
    "\n",
    "        # Keep track of progress every 10 steps\n",
    "        if n % 10 == 9 or n == steps - 1:\n",
    "            if verbose:\n",
    "                print(\"Cost after {} steps is {:.4f}\".format(n + 1, loss))\n",
    "    \n",
    "    return best_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 steps is 0.7972\n",
      "Cost after 10 steps is 0.1736\n",
      "Cost after 20 steps is 0.1116\n",
      "Cost after 30 steps is 0.0494\n",
      "Cost after 40 steps is 0.0249\n",
      "Cost after 50 steps is 0.0213\n",
      "Cost after 60 steps is 0.0176\n",
      "Cost after 70 steps is 0.0148\n",
      "Cost after 80 steps is 0.0130\n",
      "Cost after 90 steps is 0.0109\n",
      "Cost after 100 steps is 0.0086\n",
      "Cost after 110 steps is 0.0069\n",
      "Cost after 120 steps is 0.0060\n",
      "Cost after 130 steps is 0.0055\n",
      "Cost after 140 steps is 0.0052\n",
      "Cost after 150 steps is 0.0049\n",
      "Cost after 160 steps is 0.0047\n",
      "Cost after 170 steps is 0.0045\n",
      "Cost after 180 steps is 0.0042\n",
      "Cost after 190 steps is 0.0040\n",
      "Cost after 200 steps is 0.0038\n"
     ]
    }
   ],
   "source": [
    "best_params_10, _ = train(initial_states, target_states, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the initial state [0 0 0 1] we get an output with fidelity 0.9375\n",
      "From the initial state [0 0 1 0] we get an output with fidelity 0.9435\n",
      "From the initial state [0 1 0 0] we get an output with fidelity 0.9361\n",
      "From the initial state [1 0 0 0] we get an output with fidelity 0.9362\n"
     ]
    }
   ],
   "source": [
    "check_results(best_params_10, initial_states, target_states, n_layers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion 2nd circuit\n",
    "\n",
    "Much better!\n",
    "\n",
    "Using 10 layers our circuit is able to generate output states that are REALLY close to the target states. We could increase the number of layers on our system but the results wouldn't improve drastically.\n",
    "\n",
    "<font color=red>'ASK'</font>  \n",
    "Why I need to use CNOT gates if neither the initial state nor the output state has entanglement???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing layer= 2\n",
      "Computing layer= 3\n",
      "Computing layer= 4\n",
      "Computing layer= 5\n",
      "Computing layer= 6\n",
      "Computing layer= 7\n",
      "Computing layer= 8\n",
      "Computing layer= 9\n",
      "Computing layer= 10\n",
      "Computing layer= 11\n",
      "Computing layer= 12\n",
      "Computing layer= 13\n"
     ]
    }
   ],
   "source": [
    "layers = range(2, 14)\n",
    "loss_list = list()\n",
    "\n",
    "for n_layers in layers:\n",
    "    print(\"Computing layer=\", n_layers)\n",
    "    _, loss = train(initial_states, target_states, n_layers, verbose=False)\n",
    "    loss_list.append(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(layers, loss_list)\n",
    "plt.title(\"Minimum loss for each layer\")\n",
    "plt.xlabel(\"# layers\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would happen if we provided a different input state? \n",
    "\n",
    "Our model is able to perform the initial task proposed in the introduction. But what will happen if we have a different input state.\n",
    "\n",
    "- Will this input state become one of the target states?\n",
    "- Will this input state become a totally different state?\n",
    "\n",
    "In order to answer these questions, we will check if the fidelity is close to any of the target states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the initial state [0 1 1 1] we get an output with fidelity 0.0015\n",
      "From the initial state [0 1 1 1] we get an output with fidelity 0.0095\n",
      "From the initial state [0 1 1 1] we get an output with fidelity 0.0064\n",
      "From the initial state [0 1 1 1] we get an output with fidelity 0.0218\n"
     ]
    }
   ],
   "source": [
    "initial_states = [\n",
    "    np.array([0, 1, 1, 1]),\n",
    "    np.array([0, 1, 1, 1]),\n",
    "    np.array([0, 1, 1, 1]),\n",
    "    np.array([0, 1, 1, 1])\n",
    "]\n",
    "\n",
    "check_results(best_params_10, initial_states, target_states, n_layers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state $\\lvert 0111 \\rangle $ is map to a completly new state that is not close to any of the target states. Hence, our model is not overfitted to the 4 predefined targets.\n",
    "\n",
    "_For some readers this may come as no surprise since if we apply different inputs to the same transformations (Rx, Ry, CNOT) it will usually generate different outputs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Training a circuit with PyTorch: https://pennylane.ai/qml/demos/tutorial_state_preparation.html\n",
    "2. Cost function: https://pennylane.ai/qml/demos/tutorial_data_reuploading_classifier.html\n",
    "\n",
    "\n",
    "## Future steps\n",
    "\n",
    "- Train the VQA using a totally random input state.\n",
    "- Use a different cost function, try implemeting the cost function in this [tutorial](https://pennylane.ai/qml/demos/tutorial_state_preparation.html).\n",
    "- Plot probabilities of measuring different states.  \n",
    "_Although fidelity is clear to me, I think that the reader will have a better understanding of the output if I show a bar plot with probabilities_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra 1: Random input state\n",
    "\n",
    "In this section I want to see if the circuit is powerful enough to get the predefined targets using any random input state.\n",
    "\n",
    "To generate a random 4-qubit quantum state we will define a computational basis for 4-qubits and add random amplitudes ($c_0, c_1, c_2, c_3$).\n",
    "\n",
    "$$ \\psi = c_0\\lvert 0 \\rangle + c_1\\lvert 1 \\rangle + c_2\\lvert 2 \\rangle + c_3\\lvert 3 \\rangle $$\n",
    "\n",
    "Recall that the quantum state must be normalized, so we need to impose the following constraint when we generate random values: \n",
    "\n",
    "$$|c_0|^2+|c_1|^2+|c_2|^2+|c_3|^2=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2 = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "def get_angles(n_qubits, n_rotations):\n",
    "    return numpy.random.uniform(low=-np.pi, high=np.pi, size=(n_qubits, n_rotations))\n",
    "\n",
    "\n",
    "@qml.qnode(dev2, interface=\"torch\")\n",
    "def circuit_random_input(params, random_angles, fidelity_op, n_layers):\n",
    "    n_qubits = len(random_angles)\n",
    "\n",
    "    # Initial state is random\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(random_angles[i, 0], wires=i)\n",
    "        qml.RY(random_angles[i, 1], wires=i)\n",
    "    \n",
    "    for layer_lvl in range(n_layers):\n",
    "        circuit_layer(n_qubits, params, layer_lvl)\n",
    "\n",
    "    return qml.expval(qml.Hermitian(fidelity_op, wires=range(n_qubits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_random_input(params, random_angles, target_states, n_layers):\n",
    "    loss = 0\n",
    "    fidelity_operators = [density_matrix(i) for i in target_states]\n",
    "    for i in range(len(target_states)):\n",
    "        \n",
    "        fidelity = circuit_random_input(params, random_angles[i], fidelity_operators[i], n_layers)\n",
    "        loss += (1 - fidelity) ** 2\n",
    "\n",
    "    return loss / len(target_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(random_angles, target_states, n_layers, verbose=True):\n",
    "    n_qubits = initial_states[0].size\n",
    "\n",
    "    # Total of rotations gates (Rx and Ry)\n",
    "    n_rotations = 2\n",
    "    \n",
    "    # Setting intial parameters\n",
    "    params = np.random.normal(0, np.pi, (n_qubits, n_layers, n_rotations))\n",
    "    params = Variable(torch.tensor(params), requires_grad=True)\n",
    "    \n",
    "    # Computing the best parameters\n",
    "    opt = torch.optim.Adam([params], lr=0.1)\n",
    "\n",
    "    # number of steps in the optimization routine\n",
    "    steps = 200\n",
    "\n",
    "    # the final stage of optimization isn't always the best, so we keep track of the best parameters along the way\n",
    "    best_cost = cost_random_input(params, random_angles, target_states, n_layers)\n",
    "    best_params = np.zeros((n_qubits, n_layers, n_rotations))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Cost after 0 steps is {:.4f}\".format(best_cost))\n",
    "\n",
    "    # optimization begins\n",
    "    for n in range(steps):\n",
    "        opt.zero_grad()\n",
    "        loss = cost_random_input(params, random_angles, target_states, n_layers)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keeps track of best parameters\n",
    "        if loss < best_cost:\n",
    "            best_cost = loss\n",
    "            best_params = params\n",
    "\n",
    "        # Keep track of progress every 10 steps\n",
    "        if n % 10 == 9 or n == steps - 1:\n",
    "            if verbose:\n",
    "                print(\"Cost after {} steps is {:.4f}\".format(n + 1, loss))\n",
    "    \n",
    "    return best_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 steps is 0.8475\n",
      "Cost after 10 steps is 0.2007\n",
      "Cost after 20 steps is 0.0931\n",
      "Cost after 30 steps is 0.0510\n",
      "Cost after 40 steps is 0.0366\n",
      "Cost after 50 steps is 0.0273\n",
      "Cost after 60 steps is 0.0216\n",
      "Cost after 70 steps is 0.0179\n",
      "Cost after 80 steps is 0.0151\n",
      "Cost after 90 steps is 0.0130\n",
      "Cost after 100 steps is 0.0116\n",
      "Cost after 110 steps is 0.0107\n",
      "Cost after 120 steps is 0.0103\n",
      "Cost after 130 steps is 0.0100\n",
      "Cost after 140 steps is 0.0098\n",
      "Cost after 150 steps is 0.0096\n",
      "Cost after 160 steps is 0.0095\n",
      "Cost after 170 steps is 0.0094\n",
      "Cost after 180 steps is 0.0093\n",
      "Cost after 190 steps is 0.0093\n",
      "Cost after 200 steps is 0.0092\n"
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_rotations = 2\n",
    "random_angles = [\n",
    "    get_angles(n_qubits, n_rotations),\n",
    "    get_angles(n_qubits, n_rotations),\n",
    "    get_angles(n_qubits, n_rotations),\n",
    "    get_angles(n_qubits, n_rotations)\n",
    "]\n",
    "\n",
    "best_params_10_random, _ = train(random_angles, target_states, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the initial state [[-0.08855378 -1.57165424]\n",
      " [ 2.65706645  1.377499  ]\n",
      " [-0.5878746   2.14034843]\n",
      " [ 0.34437392  2.23182031]] we get an output with fidelity 0.0434\n",
      "From the initial state [[-1.2326853  -3.07984891]\n",
      " [ 0.69989569  0.94409433]\n",
      " [-1.91104108 -2.09554331]\n",
      " [ 2.66857245 -0.48458581]] we get an output with fidelity 0.0655\n",
      "From the initial state [[-0.35346274  2.5456956 ]\n",
      " [ 2.16778511 -2.53245629]\n",
      " [-1.42096702  2.19848307]\n",
      " [ 2.98342779 -3.10523836]] we get an output with fidelity 0.1660\n",
      "From the initial state [[-2.57758387 -1.37407536]\n",
      " [ 2.58046831 -1.2032229 ]\n",
      " [ 2.79449017  2.96541478]\n",
      " [-0.93978942 -0.66409163]] we get an output with fidelity 0.0590\n"
     ]
    }
   ],
   "source": [
    "def check_results(best_params, initial_states, target_states, n_layers):\n",
    "    fidelity_operators = [density_matrix(i) for i in target_states]\n",
    "    \n",
    "    for initial_state, fidelity_op in zip(initial_states, fidelity_operators):\n",
    "        fidelity = circuit_random_input(best_params, initial_state, fidelity_op, n_layers)\n",
    "        print(\"From the initial state {} we get an output with fidelity {:.4f}\".format(initial_state, fidelity))\n",
    "\n",
    "\n",
    "check_results(best_params_10_random, random_angles, target_states, n_layers=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qosf_2021_09",
   "language": "python",
   "name": "qosf_2021_09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
