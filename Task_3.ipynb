{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a variational quantum circuit with Pennylane\n",
    "\n",
    "In this notebook, we will learn how to build and train a variatioal quantum circuit (VQA) to generate a predefined 4 qubit target state from a 4 qubit random state of our choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Given the following predefined 4 qubit states: $\\lvert 0011 \\rangle$, $\\lvert 0101 \\rangle$, $\\lvert 1010 \\rangle$, $\\lvert 1100 \\rangle$. Our goal is to create a quantum circuit that can do the following mapping:\n",
    "\n",
    "- Initial random state 1, returns $\\lvert 0011 \\rangle$.\n",
    "- Initial random state 2, returns $\\lvert 0101 \\rangle$.\n",
    "- Initial random state 3, returns $\\lvert 1010 \\rangle$.\n",
    "- Initial random state 4, returns $\\lvert 1100 \\rangle$.\n",
    "\n",
    "However, for the sake of simplicity in the first section, I am going to also predefine the initial random state. In the second section, we will see how the code in the first section can be extended to solve the initial statement.\n",
    "\n",
    "$$\\lvert 0001 \\rangle ⟶  \\lvert 0011 \\rangle $$\n",
    "$$\\lvert 0010 \\rangle ⟶  \\lvert 0101 \\rangle $$\n",
    "$$\\lvert 0100 \\rangle ⟶  \\lvert 1010 \\rangle $$\n",
    "$$\\lvert 1000 \\rangle ⟶  \\lvert 1100 \\rangle $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = [\n",
    "    np.array([0, 0, 0, 1]),\n",
    "    np.array([0, 0, 1, 0]),\n",
    "    np.array([0, 1, 0, 0]),\n",
    "    np.array([1, 0, 0, 0])\n",
    "]\n",
    "\n",
    "\n",
    "target_states = [\n",
    "    np.array([0, 0, 1, 1]),\n",
    "    np.array([0, 1, 0, 1]),\n",
    "    np.array([1, 0, 1, 0]),\n",
    "    np.array([1, 1, 0, 0])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a circuit\n",
    "\n",
    "The most difficult part defining a circuit is to choose a good ansatz. Since I don't have a clue of what will work best I will implement a general circuit parametrized. Using 2 rotational qubit gates (Rx and Ry) I can generate all possible state in the Bloch Sphere and to generate entanglement I will implement several CNOT gate to entangle all the qubits. \n",
    "\n",
    "I will repeat this layer (rotation gates + CNOT gates) 2 times. The number of layers is an hyperparameter that we may need to tune later.\n",
    "\n",
    "**Important:**  We need to set the parameters for the rotational gates, since I want to generate all possible angles, I will generate random numbers from $0$ to $\\pi$.\n",
    "\n",
    "<font color=red>'NOT COMPLETLY SURE'</font>  \n",
    "In the [tutorial](https://pennylane.ai/qml/demos/tutorial_state_preparation.html) generates random numbers from 0 to pi. This is enough to generat all possible states in the bloch sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "n_rotations = 2\n",
    "\n",
    "# randomly initialize parameters from a normal distribution\n",
    "params = np.random.normal(0, np.pi, (n_qubits, n_layers, n_rotations))\n",
    "params = Variable(torch.tensor(params), requires_grad=True)\n",
    "\n",
    "def circuit_layer(n_qubits, params, layer_lvl):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(params[i, layer_lvl, 0], wires=i)\n",
    "        qml.RY(params[i, layer_lvl, 1], wires=i)\n",
    "\n",
    "    # Entangling all qubits\n",
    "    qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our circuit in 3 steps: \n",
    "\n",
    "\n",
    "**1. Initialize in the right state.**  \n",
    "All circuit start with all qubits in the zero state, $\\lvert 0\\rangle$. Using ```qml.BasisState``` our circuit will start with the state of our choice: $\\lvert 0001 \\rangle$ or $\\lvert 0010 \\rangle$, etc.\n",
    "\n",
    "**2. Implement the layer.**  \n",
    "Using a simple for loop we can add as many layers as we want.\n",
    "\n",
    "**3. Measurement.**    \n",
    "In this last step, we will measure the **fidelity** between the **output state** and **target state**. The fidelity is a value between 0 (orthogonal) or 1 (equal) that measure if the closeness between two states, in other words it measures if both states are similar or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def circuit(params, initial_state, fidelity_op, n_layers):\n",
    "    n_qubits = len(initial_state)\n",
    "    qml.BasisState(initial_state, wires=range(n_qubits))\n",
    "    \n",
    "    for layer_lvl in range(n_layers):\n",
    "        circuit_layer(n_qubits, params, layer_lvl)\n",
    "\n",
    "    return qml.expval(qml.Hermitian(fidelity_op, wires=range(n_qubits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a cost function\n",
    "\n",
    "The cost function helps the circuit to find the target state. In this case a good cost function will be the fidelity, so we can simply define our cost funtion as:\n",
    "\n",
    "$$ C(\\theta) = \\sum \\big( 1-\\langle \\psi_o(\\theta) \\lvert \\psi_t \\rangle \\big) ^2 $$\n",
    "\n",
    "where $\\theta$ are the variational parameters that the circuit needs to tune, $\\psi_o$ is the output state, $\\psi_t$ is one of the target states that we have define in the begining and $\\langle \\psi_o(\\theta) \\lvert \\psi_t \\rangle$ is how the fidelity is computed.\n",
    "\n",
    "\n",
    "### Explaining fidelity_op (Extra)\n",
    "\n",
    "_I have written this subsection in case someone needs to understand how to implement the fidelity operator._\n",
    "\n",
    "Suppose I have the inital state $\\lvert 00 \\rangle$ and our target is $\\lvert 01 \\rangle$. If our quantum circuit doesn't works properly it can outputs states like:\n",
    "- $\\lvert 10 \\rangle$\n",
    "- $\\lvert 11 \\rangle$.  \n",
    "\n",
    "But which state is closer to the target? In order to answer this question we can compute the fidelity operator of our target state ($\\lvert 01 \\rangle \\langle 01 \\rvert$).\n",
    "$$ \\lvert 01 \\rangle \\langle 01 \\rvert = \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "And measure how close each state is:  \n",
    "**State  $\\lvert 10 \\rangle$**:\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "0  \\\\\n",
    "\\end{pmatrix}\n",
    "= 0$$ \n",
    "\n",
    "**State  $\\lvert 11 \\rangle$**:\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{pmatrix}\n",
    "= 0$$ \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Even though state $\\lvert 11 \\rangle$ is closer to $\\lvert 01 \\rangle$ (there is only 1 qubit wrong), the fidelity interprets that both $\\lvert 11 \\rangle$ and $\\lvert 10 \\rangle$ are far away from $\\lvert 01 \\rangle$.\n",
    "\n",
    "<font color=red>'ASK'</font>  \n",
    "So perhaps, fidelity is not a good cost function for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_product(matrices: list):\n",
    "    if len(matrices)==2:\n",
    "        return np.kron(matrices[0], matrices[1], requires_grad=False)\n",
    "    return np.kron(matrices[0], tensor_product(matrices[1:]), requires_grad=False)\n",
    "\n",
    "\n",
    "def density_matrix(state):\n",
    "    \"\"\"Computes the density matrix of a state. \n",
    "    Given the state |0001> it will compute |0001><0001|\n",
    "    \n",
    "    Args:\n",
    "        state (np.array): array representing a quantum state vector\n",
    "    \n",
    "    Returns:\n",
    "        density_matrix (np.array): array representing the density matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    zero_state = np.array([1, 0], requires_grad=False)\n",
    "    one_state = np.array([0, 1], requires_grad=False)\n",
    "    matrices = [zero_state if qubit_state == 0 else one_state for qubit_state in state]\n",
    "    vector = tensor_product(matrices)\n",
    "    return np.outer(vector, np.conj(vector), requires_grad=False)\n",
    "\n",
    "\n",
    "def cost(params, initial_states, target_states, n_layers):\n",
    "    loss = 0\n",
    "    fidelity_operators = [density_matrix(i) for i in target_states]\n",
    "    for i in range(len(target_states)):\n",
    "\n",
    "        fidelity = circuit(params, initial_states[i], fidelity_operators[i], n_layers)\n",
    "        loss += (1 - fidelity) ** 2\n",
    "\n",
    "    return loss / len(target_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best parameters I use the code that you can find in this [tutorial](https://pennylane.ai/qml/demos/tutorial_state_preparation.html). We simply set a classical optimizer that find the best parameters to minimize the previous cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 steps is 0.9009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/gllodra12/extended_linux/Projectes/Quantum_projects/qosf/qosf_2021_09/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:147: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:240.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 10 steps is 0.6261\n",
      "Cost after 20 steps is 0.3889\n",
      "Cost after 30 steps is 0.3980\n",
      "Cost after 40 steps is 0.3778\n",
      "Cost after 50 steps is 0.3779\n",
      "Cost after 60 steps is 0.3753\n",
      "Cost after 70 steps is 0.3753\n",
      "Cost after 80 steps is 0.3751\n",
      "Cost after 90 steps is 0.3750\n",
      "Cost after 100 steps is 0.3750\n",
      "Cost after 110 steps is 0.3750\n",
      "Cost after 120 steps is 0.3750\n",
      "Cost after 130 steps is 0.3750\n",
      "Cost after 140 steps is 0.3750\n",
      "Cost after 150 steps is 0.3750\n",
      "Cost after 160 steps is 0.3750\n",
      "Cost after 170 steps is 0.3750\n",
      "Cost after 180 steps is 0.3750\n",
      "Cost after 190 steps is 0.3750\n",
      "Cost after 200 steps is 0.3750\n"
     ]
    }
   ],
   "source": [
    "# set up the optimizer\n",
    "opt = torch.optim.Adam([params], lr=0.1)\n",
    "\n",
    "# number of steps in the optimization routine\n",
    "steps = 200\n",
    "\n",
    "# the final stage of optimization isn't always the best, so we keep track of\n",
    "# the best parameters along the way\n",
    "best_cost = cost(params, initial_states, target_states, n_layers)\n",
    "best_params = np.zeros((n_qubits, n_layers, n_rotations))\n",
    "\n",
    "print(\"Cost after 0 steps is {:.4f}\".format(best_cost))\n",
    "\n",
    "# optimization begins\n",
    "for n in range(steps):\n",
    "    opt.zero_grad()\n",
    "    loss = cost(params, initial_states, target_states, n_layers)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # keeps track of best parameters\n",
    "    if loss < best_cost:\n",
    "        best_cost = loss\n",
    "        best_params = params\n",
    "\n",
    "    # Keep track of progress every 10 steps\n",
    "    if n % 10 == 9 or n == steps - 1:\n",
    "        print(\"Cost after {} steps is {:.4f}\".format(n + 1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results don't look really promising, perhaps we will need to increase the number of layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the initial state [0 0 0 1] we get an output with fidelity 0.0000\n",
      "From the initial state [0 0 1 0] we get an output with fidelity 0.5000\n",
      "From the initial state [0 1 0 0] we get an output with fidelity 0.5000\n",
      "From the initial state [1 0 0 0] we get an output with fidelity 0.9996\n"
     ]
    }
   ],
   "source": [
    "def check_results(best_params, initial_states, target_states, n_layers):\n",
    "    fidelity_operators = [density_matrix(i) for i in target_states]\n",
    "    \n",
    "    for initial_state, fidelity_op in zip(initial_states, fidelity_operators):\n",
    "        fidelity = circuit(best_params, initial_state, fidelity_op, n_layers)\n",
    "        print(\"From the initial state {} we get an output with fidelity {:.4f}\".format(initial_state, fidelity))\n",
    "\n",
    "check_results(best_params, initial_states, target_states, n_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion 1st circuit\n",
    "\n",
    "Our circuit is not able generate the target states. We only get a good result with the input $\\lvert 1000 \\rangle$ because its fidelity is close to 1, which means that the output state is really close to the target state, $\\lvert 1100 \\rangle$.\n",
    "\n",
    "Let's see if increasing the number of layers our results will improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(initial_states, target_states, n_layers, verbose=True):\n",
    "    n_qubits = initial_states[0].size\n",
    "\n",
    "    # Total of rotations gates (Rx and Ry)\n",
    "    n_rotations = 2\n",
    "    \n",
    "    # Setting intial parameters\n",
    "    params = np.random.normal(0, np.pi, (n_qubits, n_layers, n_rotations))\n",
    "    params = Variable(torch.tensor(params), requires_grad=True)\n",
    "    \n",
    "    # Computing the best parameters\n",
    "    opt = torch.optim.Adam([params], lr=0.1)\n",
    "\n",
    "    # number of steps in the optimization routine\n",
    "    steps = 200\n",
    "\n",
    "    # the final stage of optimization isn't always the best, so we keep track of the best parameters along the way\n",
    "    best_cost = cost(params, initial_states, target_states, n_layers)\n",
    "    best_params = np.zeros((n_qubits, n_layers, n_rotations))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Cost after 0 steps is {:.4f}\".format(best_cost))\n",
    "\n",
    "    # optimization begins\n",
    "    for n in range(steps):\n",
    "        opt.zero_grad()\n",
    "        loss = cost(params, initial_states, target_states, n_layers)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keeps track of best parameters\n",
    "        if loss < best_cost:\n",
    "            best_cost = loss\n",
    "            best_params = params\n",
    "\n",
    "        # Keep track of progress every 10 steps\n",
    "        if n % 10 == 9 or n == steps - 1:\n",
    "            if verbose:\n",
    "                print(\"Cost after {} steps is {:.4f}\".format(n + 1, loss))\n",
    "    \n",
    "    return best_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 steps is 0.8472\n",
      "Cost after 10 steps is 0.0996\n",
      "Cost after 20 steps is 0.0303\n",
      "Cost after 30 steps is 0.0180\n",
      "Cost after 40 steps is 0.0101\n",
      "Cost after 50 steps is 0.0062\n",
      "Cost after 60 steps is 0.0050\n",
      "Cost after 70 steps is 0.0043\n",
      "Cost after 80 steps is 0.0038\n",
      "Cost after 90 steps is 0.0035\n",
      "Cost after 100 steps is 0.0033\n",
      "Cost after 110 steps is 0.0031\n",
      "Cost after 120 steps is 0.0029\n",
      "Cost after 130 steps is 0.0028\n",
      "Cost after 140 steps is 0.0026\n",
      "Cost after 150 steps is 0.0026\n",
      "Cost after 160 steps is 0.0025\n",
      "Cost after 170 steps is 0.0024\n",
      "Cost after 180 steps is 0.0023\n",
      "Cost after 190 steps is 0.0023\n",
      "Cost after 200 steps is 0.0022\n"
     ]
    }
   ],
   "source": [
    "best_params_10, _ = train(initial_states, target_states, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the initial state [0 0 0 1] we get an output with fidelity 0.9480\n",
      "From the initial state [0 0 1 0] we get an output with fidelity 0.9599\n",
      "From the initial state [0 1 0 0] we get an output with fidelity 0.9563\n",
      "From the initial state [1 0 0 0] we get an output with fidelity 0.9488\n"
     ]
    }
   ],
   "source": [
    "check_results(best_params_10, initial_states, target_states, n_layers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion 2nd circuit\n",
    "\n",
    "Much better!\n",
    "\n",
    "Using 10 layers our circuit is able to generate output states that are REALLY close to the target states. We could increase the number of layers on our system but the results wouldn't improve drastically.\n",
    "\n",
    "<font color=red>'ASK'</font>  \n",
    "Why I need to use CNOT gates if neither the initial state nor the output state has entanglement???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing layer= 2\n",
      "Computing layer= 3\n",
      "Computing layer= 4\n",
      "Computing layer= 5\n",
      "Computing layer= 6\n",
      "Computing layer= 7\n",
      "Computing layer= 8\n",
      "Computing layer= 9\n",
      "Computing layer= 10\n",
      "Computing layer= 11\n",
      "Computing layer= 12\n",
      "Computing layer= 13\n"
     ]
    }
   ],
   "source": [
    "layers = range(2, 14)\n",
    "loss_list = list()\n",
    "\n",
    "for n_layers in layers:\n",
    "    print(\"Computing layer=\", n_layers)\n",
    "    _, loss = train(initial_states, target_states, n_layers, verbose=False)\n",
    "    loss_list.append(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3089f67f70>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(layers, loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Training a circuit with PyTorch: https://pennylane.ai/qml/demos/tutorial_state_preparation.html\n",
    "2. Cost function: https://pennylane.ai/qml/demos/tutorial_data_reuploading_classifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra 1: Random input state\n",
    "\n",
    "In this section we will explore if the same circuit is able to get good results on random input states.  \n",
    "\n",
    "To generate a random 4-qubit quantum state we will define a computational basis for 4-qubits and add random amplitudes ($c_0, c_1, c_2, c_3$).\n",
    "\n",
    "$$ \\psi = c_0\\lvert 0 \\rangle + c_1\\lvert 1 \\rangle + c_2\\lvert 2 \\rangle + c_3\\lvert 3 \\rangle $$\n",
    "\n",
    "Recall that the quantum state must be normalize, so we need to impose the following constraint when we generate random values: \n",
    "\n",
    "$$|c_0|^2+|c_1|^2+|c_2|^2+|c_3|^2=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vull que siguin complexos i random entre -1 i 1 i que entre -1 i 1 la distribucio sigui uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qosf_2021_09",
   "language": "python",
   "name": "qosf_2021_09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
